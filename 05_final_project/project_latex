\documentclass{beamer}

\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usetheme{Copenhagen}


%Information to be included in the title page:
\title{DL Lab Course: Final Project}
\author{Yufeng Xiong}
\institute{University of Freiburg}
\date{Feb 17th, 2017}


\begin{document}

\frame{\titlepage}

\begin{frame}
\frametitle{Project Purpose}

Deep Q-Network (DQN) equation for updating the target value:
\begin{gather*}
Q_{target} = r + \gamma * \max\limits_{a'} Q(s', a', \alert{w})
\end{gather*}

Problems:
\begin{itemize}
\item Use the \alert{same network} to evaluate and select the action
\item Lead to \alert{overestimation} of the actions (positive bias)
\item \alert{Harm performance and stability}
\end{itemize}

Thus, \alert{Double DQN} is proposed.

\end{frame}

\begin{frame}
\frametitle{Double Deep Q-Network (DDQN)}
In DDQN, a target network is proposed to \alert{remove the upward bias} caused by $\max\limits_{a} Q(s, a, w)$.
\begin{itemize}
\item Primary network ($\alert{w}$) is uesed to choose an action
\item Target network ($\alert{w^{-}}$) is used to evaluate the target Q-value
\end{itemize}

DDQN equation for updating the target value:
\begin{gather*}
Q_{target} = r + \gamma * Q(s', arg \max\limits_{a'} Q(s', a', \alert{w}), \alert{w^{-}})
\end{gather*}

Then the MSE loss equation becomes:
\begin{gather*}
I = (r + \gamma * Q(s', arg \max\limits_{a'} Q(s', a', \alert{w}), \alert{w^{-}}) - Q(s, a, w))^{2}
\end{gather*}
\end{frame}

\begin{frame}
\frametitle{Implemention details}
I implement the DDQN for the simple-maze task in assignment 4 and compare the results with DQN.

\alert{The network architecture is}:
\begin{itemize}
\item Convolutional layer 1
\item Convolutional layer 2
\item Convolutional layer 3
\item Fully connected layer 1
\item Fully connected layer 2
\end{itemize}

\alert{Key parameters}:
\begin{itemize}
\item training steps: 100,000
\item test steps:     30,000
\item target Q-network update: every 1000 steps
\item epsilon: 0.2
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Performance Comparisons}
The comparative results are as follows:
\includegraphics [scale=0.7] {performance.png}

In conclusion, by decoupling the action choice from the target
Q-value generation, the improved DQN can substantially \alert{reduce
the overestimation}, \alert{train faster} and \alert{be more stable}.

\end{frame}

\begin{frame}
\frametitle{References}
\begin{itemize}
\item Van Hasselt, H., Guez, A. and Silver, D., 2016, March. Deep Reinforcement Learning with Double Q-Learning. In AAAI (pp. 2094-2100).
\item $icml.cc/2016/tutorials/deep\_rl\_tutorial.pdf$
\end{itemize}
\end{frame}

\end{document}

